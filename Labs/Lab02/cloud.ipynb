{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To The Cloud (AND BEYOND)\n",
    "Now that we have a good idea on how to solve the digit problem, it's time to move the training part to the cloud (again, there's no need to with this problem but for other problems we test locally to test things out on a subset of our data and move to the cloud to work on the whole lot).\n",
    "\n",
    "Let's set some things up!\n",
    "\n",
    "The first thing you might need to do is ensure that our `azureml.core` package is installed in the notebook environment. If you are using Azure Notebooks there's an easy two step process to get going."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Dependencies in Azure Notebooks\n",
    "Click on the \"Project Settings\"\n",
    "\n",
    "![Project Setings](https://raw.githubusercontent.com/sethjuarez/pytorchintro/master/images/project_settings.png)\n",
    "\n",
    "Next, select the \"Environments\" tab, choose \"Python 3.6\", and finally select the corresponding `requirements.txt` file.\n",
    "\n",
    "![Settings](https://raw.githubusercontent.com/sethjuarez/pytorchintro/master/images/settings.png)\n",
    "\n",
    "After those steps you should be good to go!\n",
    "\n",
    "NOTE: If you have an issue after setting up the project settings. In the notebook make sure the kernel is set to python 3.6 by doing the following: Select Kernel> Change Kernel >  Python 3.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import azureml\n",
    "from azureml.core.model import Model\n",
    "from azureml.core import Workspace, Run, Experiment\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.train.dnn import PyTorch\n",
    "from azureml.widgets import RunDetails\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Azure Machine Learning service\n",
    "The first thing you need to do is create an Azure Machine Learning workspace. There are [docs](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-get-started#create-a-workspace) on how to do that. If you're a command line type person, I have an [example](https://github.com/sethjuarez/workspacestarter) of how you can set it up using the Azure CLI. Once you've set the project up fill in the appropriate settings for your workspace by uncommenting the code below to write out the config file. Once the config file has been written out, you can load the workspace programmatically like I've done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##use this code to set up config file\n",
    "#subscription_id ='<SUB_ID>'\n",
    "#resource_group ='<RESOURCE_GROUP>'\n",
    "#workspace_name = '<WORKSPACE>'\n",
    "\n",
    "#try:\n",
    "#    ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
    "#    ws.write_config()\n",
    "#    print('Workspace configuration succeeded. You are all set!')\n",
    "#except:\n",
    "#    print('Workspace not found. TOO MANY ISSUES!!!')\n",
    "\n",
    "##once you run the above code once, you can use the written config\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Compute\n",
    "Next we need to define a compute target for your experiment. Since this is a brand new workspace, feel free to change the name of your cluster (I called my `racer`). The code below tries to get a reference to my cluster but if it doesn't exist, it creates it for me. If you're creating a cluster this might take a bit of time. Also, please turn these off when you're done (in fact consider setting the `min_nodes` to 0 so the cluster turns off automatically if it's idle for too long) - I don't want you to get an unexpected bill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = 'racer'\n",
    "try:\n",
    "    compute = ComputeTarget(workspace=ws, name=cluster)\n",
    "    print('Found existing compute target \"{}\"'.format(cluster))\n",
    "except ComputeTargetException:\n",
    "    print('Creating new compute target \"{}\"...'.format(cluster))\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6', min_nodes=1, max_nodes=6)\n",
    "    compute = ComputeTarget.create(ws, cluster, compute_config)\n",
    "    compute.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to Experiment\n",
    "Once our compute target has been set up it's time to package up our tiny notebook from last time into a single script that a remote compute environment can run. I've taken the time to [do that for you](train.py). In fact, if you look at the file you will see all of the exact same concepts we learned from the previous notebook (it's almost exactly the same but I have added additional things to make it easier to pass things into the script).\n",
    "\n",
    "In AzureML service there is a concept of an experiment. For every experiment you can have multiple runs. In this case I'm using an `Estimator` object that defines how the experiment should run.\n",
    "\n",
    "### Don't read this if you don't care what we do in the background\n",
    "In the background the estimator is basically a definition of sorts for a docker image that will house your experiment. The best part about all of this is that irrespective of what you use for your experiment (a crazy custom version of TensorFlow or something) it should always run - it's a container after all. It's pretty slick.\n",
    "\n",
    "### Back to the regular stuff\n",
    "Once we submit our estimator to be run on AzureML service, it copies the contents of the current directory and packages them up to run in our new container (well, it will upload everything with the exception of anything you put describe in the [.amlignore](https://github.com/sethjuarez/pytorchintro/blob/master/.amlignore) file).\n",
    "\n",
    "Notice also that since I'm using `argparse` I can specify external parameters to the training script as part of the estimator definition.\n",
    "\n",
    "Let's run the next three lines to see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run experiment\n",
    "mnist = Experiment(ws, 'pytorchmnist')\n",
    "\n",
    "# script parameters\n",
    "script_params={\n",
    "    '--epochs': 5,\n",
    "    '--batch': 100,\n",
    "    '--lr': .001,\n",
    "    '--model': 'cnn'\n",
    "}\n",
    "\n",
    "# Create Estimator\n",
    "estimator = PyTorch(source_directory='.',\n",
    "                       compute_target=compute, \n",
    "                       entry_script='train.py',\n",
    "                       script_params=script_params,\n",
    "                       use_gpu=True)\n",
    "\n",
    "run = mnist.submit(estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything wen't to plan you should see something like this:\n",
    "\n",
    "![AzureML Run](https://raw.githubusercontent.com/sethjuarez/pytorchintro/master/images/run_widget.png)\n",
    "\n",
    "Notice that indeed the loss function decreased (on average) over time and the accuracy of the model increased! Try playing around with the `learning_rate` by changing the parameters. Better yet, you can have [AzureML service sweep accross a whole bunch of parameters](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters) for you!\n",
    "\n",
    "Now for the question of how I got those wonderful charts to show up. This is where AzureML service starts to actually add value to what you were already doing. With a [few](https://github.com/sethjuarez/pytorchintro/blob/master/train.py#L156-L166) [strategically](https://github.com/sethjuarez/pytorchintro/blob/master/train.py#L121-L122) [placed](https://github.com/sethjuarez/pytorchintro/blob/master/train.py#L142-L143) log statements AzureML service was able to create this output. In fact, if a value is logged more than once it automatically creates charts instead of items in a table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "Once the training is all done and you're satisfied with the output, you can actually peruse the ouput of all of the runs for a given experiment and promote it to an \"official\" workspace model. This is an awesome feature because the important files (i.e. the model that will make us zillionaires) are usually sitting on the computer some dude named Jeff. Also, many people don't even version models nowadays - running the code below will!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_file_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = 'outputs/model.pth'\n",
    "run.download_file(name=model_file, output_file_path='model.pth')\n",
    "model = Model.register(ws, model_name='PyTorchMNIST', model_path='model.pth', \n",
    "                       description='CNN PyTorch Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Image\n",
    "Now that we have the model, if we want to use it in production we need to define how one should use the model. This is sometimes called scoring or inferencing. For AzureML service we are basically looking for two functions:\n",
    "1. `init()`, and\n",
    "2. `run(raw)` which takes in a json string and returns a prediction\n",
    "First thing's first though - we need to describe the environment where the scoring script will live and package it up into an environment file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myenv = CondaDependencies()\n",
    "myenv.add_pip_package('numpy')\n",
    "myenv.add_pip_package('torch')\n",
    "with open('pytorchmnist.yml','w') as f:\n",
    "    print('Writing out {}'.format('pytorchmnist.yml'))\n",
    "    f.write(myenv.serialize_to_string())\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to tell AzureML service the location of the scoring script. I have taken the liberty, dear reader, to [create that for you](score.py). Looking through the file you should be able to easily find both the `init()` and `run(raw)` methods. You can also run the file locally to make sure it is doing the right thing.\n",
    "\n",
    "Now that we have everything let's create an image!\n",
    "\n",
    "### Don't read this if you don't want to know what is happening in the background\n",
    "What we basically do is create a docker image from your definition and push it up to the an Azure Container Registry that belogns to the Workspace.\n",
    "\n",
    "**NOTE** This takes a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.image import ContainerImage, Image\n",
    "\n",
    "# create image\n",
    "image_config = ContainerImage.image_configuration(execution_script=\"score.py\", \n",
    "                                runtime=\"python\", \n",
    "                                conda_file=\"pytorchmnist.yml\")\n",
    "\n",
    "image = Image.create(ws, 'pytorchmnist', [model], image_config)\n",
    "image.wait_for_creation(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy!!\n",
    "You could have certainly stopped with creating the image and moving the rest of the deployment process to something like Azure Pipelines. If you want to continue to deploy this service to the Workspace, this is how you do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import Webservice, AciWebservice\n",
    "\n",
    "service_name = 'pytorchmnist-svc'\n",
    "\n",
    "# check for existing service\n",
    "svcs = [svc for svc in Webservice.list(ws) if svc.name==service_name]\n",
    "if len(svcs) == 1:\n",
    "    print('Deleting prior {} deployment'.format(service_name))\n",
    "    svcs[0].delete()\n",
    "\n",
    "# create service\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                            memory_gb=1, \n",
    "                                            description='simple MNIST digit detection')\n",
    "service = Webservice.deploy_from_image(workspace=ws, \n",
    "                                    image=image, \n",
    "                                    name=service_name, \n",
    "                                    deployment_config=aciconfig)\n",
    "service.wait_for_deployment(show_output=True)\n",
    "print(service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have the option of pushing the image to ACI or even a workspace Kubernetes cluster.\n",
    "\n",
    "Sometimes things go wrong....... If it does for you run the code below to see the actual [logs](deploy.log)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('deploy.log','w') as f:\n",
    "    f.write(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Service\n",
    "IT'S ALIVE!!! Let's see if it does sensible things. We will load up the test data from before so we can try random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.MNIST('data', train=False, download=True,\n",
    "                        transform=transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Lambda(lambda x: x.reshape(28*28))\n",
    "                        ])\n",
    "                     )\n",
    "print(len(digits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can basically choose any number as an index up to 60,000 (well, one less). Try out a couple to see how the service does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, Y = digits[20]\n",
    "X = X * 255\n",
    "plt.imshow(255 - X.reshape(28,28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is a string representation of the image we will POST to the endpoint\n",
    "image_str = ','.join(map(str, X.int().tolist()))\n",
    "print(image_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "service_url = service.scoring_uri\n",
    "print(service_url)\n",
    "r = requests.post(service_url, json={'image': image_str })\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Thoughts\n",
    "Hopefully this little journey was helpful! My goal is to show you that tha basics of Machine Learning are not all that bad. If you have any comments, suggestions, or something does not make sense make sure to drop me a line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
